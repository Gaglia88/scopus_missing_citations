{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "117666af",
   "metadata": {},
   "source": [
    "# Scopus missing citations locator\n",
    "This notebook can help you to find missing citations on Scopus that are indexed on Google Scholar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b8b548",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29be064",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--incognito\")\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), chrome_options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa37a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Returns the publications of a specific author on Scholar\n",
    "    driver: selenium driver\n",
    "    author_id: id of the author\n",
    "    wait_timeout: timeout to load all the articles\n",
    "\"\"\"\n",
    "def getScholarArticles(driver, author_id, wait_timeout=10):\n",
    "    pub_scholar = []\n",
    "    pub_id = 0\n",
    "    \n",
    "    #Load the page of the author\n",
    "    driver.get(\"http://scholar.google.it/citations?user=\"+author_id)\n",
    "    #Locate the button \"more\" to get all the publications\n",
    "    btnMore = driver.find_element(By.ID, 'gsc_bpf_more')\n",
    "    #Load all the publications\n",
    "    while btnMore.is_enabled():\n",
    "        #Click on the button \"more\"\n",
    "        btnMore.click()\n",
    "        #Wait until the page is loaded\n",
    "        time.sleep(wait_timeout)\n",
    "\n",
    "    #Foreach row in the table\n",
    "    for tr in driver.find_elements(By.XPATH, \"//tr[@class='gsc_a_tr']\"):\n",
    "        pub = dict()\n",
    "        #Extract the publication info\n",
    "        td_info = tr.find_element(By.XPATH, \"./td[@class='gsc_a_t']\")\n",
    "        tmp = td_info.find_elements(By.XPATH, \"./div[@class='gs_gray']\")\n",
    "        pub['pub_id'] = pub_id\n",
    "        pub_id += 1\n",
    "        pub['title'] = td_info.find_element(By.XPATH, \"./a[@class='gsc_a_at']\").text\n",
    "        pub['authors'] = tmp[0].text\n",
    "        pub['venue'] = tmp[1].text\n",
    "        \n",
    "        #Extract the citations\n",
    "        pub['num_cit'] = 0\n",
    "        #URL to articles that cite this page\n",
    "        pub['cit_url'] = \"\"\n",
    "\n",
    "        try:\n",
    "            td_n_cit = tr.find_element(By.XPATH, \"./td[@class='gsc_a_c']\")\n",
    "            a = td_n_cit.find_element(By.TAG_NAME, \"a\")\n",
    "            pub['cit_url'] = a.get_attribute('href')\n",
    "            pub['num_cit'] = a.text\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        pub_scholar.append(pub)\n",
    "        pass\n",
    "    return pub_scholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25272980",
   "metadata": {},
   "outputs": [],
   "source": [
    "done = []\n",
    "citations = []\n",
    "\"\"\"\n",
    "Given a dataframe of publications generated with getScholarArticles\n",
    "generates the list of their citations\n",
    "\"\"\"\n",
    "def extractCitationsScholar(driver, pub_scholar):\n",
    "    global done\n",
    "    global citations\n",
    "    \n",
    "    #For each publication\n",
    "    for pub in pub_scholar:\n",
    "        if pub['pub_id'] not in done:\n",
    "            if len(pub[\"cit_url\"]) > 0:\n",
    "                #Open the main page related to citations\n",
    "                driver.get(pub[\"cit_url\"])\n",
    "                #Get the links of the subpages\n",
    "                pages = []\n",
    "                try:\n",
    "                    pages = [a.get_property('href') for a in driver.find_element(By.XPATH, \"//div[@id='gs_nml']\").find_elements(By.XPATH, \"./a[@class='gs_nma']\")]\n",
    "                except:\n",
    "                    pass\n",
    "                #For each subpage, starting by the first one\n",
    "                i = 0\n",
    "                while i < len(pages)+1:\n",
    "                    #Extracts the citations for the current page\n",
    "                    for cit in driver.find_element(By.XPATH, \"//div[@id='gs_res_ccl_mid']\").find_elements(By.XPATH, \"//div[@class='gs_r gs_or gs_scl']\"):\n",
    "                        citation = dict()\n",
    "                        citation['pub_id'] = pub['pub_id']\n",
    "                        try:\n",
    "                            citation['cit_name'] = cit.find_element(By.XPATH, \"./div[@class='gs_ri']/h3/a\").text    \n",
    "                            info = cit.find_element(By.XPATH, \"./div[@class='gs_ri']/div[@class='gs_a']\").text\n",
    "                            citation['cit_authors'] = info.split('-')[0]\n",
    "                            citation['cit_venue'] = ' - '.join(info.split('-')[1:])\n",
    "                            cit.find_element(By.XPATH, \"//a[@aria-controls='gs_cit']\").click()\n",
    "                            time.sleep(0.2)\n",
    "                            citation['bibtex_url'] = driver.find_element(By.XPATH, '//a[@class=\"gs_citi\"]').get_attribute('href')\n",
    "                            time.sleep(0.2)\n",
    "                            driver.find_element(By.XPATH, '//a[@id=\"gs_cit-x\"]').click()\n",
    "                            time.sleep(0.2)\n",
    "                        except:\n",
    "                            citation['cit_text'] = cit.text\n",
    "                        citations.append(citation)\n",
    "                    #Navigate to the next page\n",
    "                    if i < len(pages):\n",
    "                        driver.get(pages[i])\n",
    "                    i += 1\n",
    "            done.append(pub['pub_id'])\n",
    "    return citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c49d803",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compute the Jaccard similarity of two strings\n",
    "\"\"\"\n",
    "def js(s1, s2):\n",
    "    s1 = set(re.split('\\W+', str(s1).lower().strip()))\n",
    "    s2 = set(re.split('\\W+', str(s2).lower().strip()))\n",
    "    com = float(len(s1.intersection(s2)))\n",
    "    return com / (len(s1)+len(s2)-com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cf2679",
   "metadata": {},
   "source": [
    "# Step 1 - Extract articles and citations from Scholar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98f75e8",
   "metadata": {},
   "source": [
    "## Download all the articles from your Scholar profile\n",
    "You can find your author_id in the URL of your Google Scholar page: is the code after '?user='.\n",
    "\n",
    "For example, in the URL https://scholar.google.it/citations?user=lACV6IYAAAAJ the author_id is 'lACV6IYAAAAJ'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5484643",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Author id\n",
    "author_id = \"lACV6IYAAAAJ\"\n",
    "#Returns the list of articles\n",
    "pub_scholar = getScholarArticles(driver, author_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b974590",
   "metadata": {},
   "source": [
    "Store the articles in a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4e5339",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pub_scholar).to_csv('pub_scholar.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8d3866",
   "metadata": {},
   "source": [
    "## Download the citations\n",
    "Download all the citations of the previous extracted articles.\n",
    "\n",
    "If Scholar asks if you are a robot, do the procedure, and then run the cell again, until all the citations are extracted.\n",
    "It can happen multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fc4c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations_scholar = extractCitationsScholar(driver, pub_scholar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7936e7",
   "metadata": {},
   "source": [
    "Store the citations in a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14e11cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(citations_scholar).to_csv('cit_scholar.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c6a64e",
   "metadata": {},
   "source": [
    "# Step 2 - Download your citations from Scopus\n",
    "\n",
    "Go on your page on Scopus, then go on the 'Cited by XX documents' tab, and click on the arrow at the right of 'Export all to CSV File'.\n",
    "\n",
    "![image](images/scopus.png)\n",
    "\n",
    "A new window will open, select 'CSV' as export method, check 'include references', finally click on 'Export'.\n",
    "\n",
    "![image](images/scopus2.png)\n",
    "\n",
    "Scopus will generate a CSV file called 'scopus.csv' that must be placed in the same folder of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eef2cb",
   "metadata": {},
   "source": [
    "# Step 3 - Looking for missing citations\n",
    "\n",
    "This step tries to perform Entity Resolution on the citations looking for those that do not have a correspondence on Scopus.\n",
    "The process is relatively raw and could be improved, but works quite well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f09d52",
   "metadata": {},
   "source": [
    "## Simplest approach\n",
    "Matching only on the title of the citations. In short, it performs a similarity join between Scholar and Scopus citations, keeping only those from Scholar that do not have a correspondence in Scopus.\n",
    "\n",
    "Every row of the output dataframe contains:\n",
    "* **cit_name**: title of the missing citation;\n",
    "* **cit_authors**: authors of the missing citation;\n",
    "* **cit_venue**: venue of the missing citation;\n",
    "* **title**: title of the article on Scopus on which the citation is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7ec955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scholar articles\n",
    "pub_scholar = pd.read_csv('pub_scholar.csv')\n",
    "# Scholar citations\n",
    "cit_scholar = pd.read_csv('cit_scholar.csv')\n",
    "# Scopus citations\n",
    "cit_scopus = pd.read_csv('scopus.csv', sep=\",\")\n",
    "\n",
    "# Add a unique identifier to each citation\n",
    "cit_scholar = cit_scholar.reset_index().rename({'index':'scholar_id'}, axis=1)\n",
    "cit_scopus = cit_scopus.reset_index().rename({'index':'scopus_id'}, axis=1)\n",
    "\n",
    "# Create a pair \"citation id - Title\"\n",
    "cit_scholar_1 = cit_scholar[['scholar_id', 'cit_name']]\n",
    "cit_scopus_1 = cit_scopus[['scopus_id', 'Title']]\n",
    "\n",
    "# Perform the cartesian product\n",
    "cross = cit_scopus_1.merge(cit_scholar_1, how='cross')\n",
    "\n",
    "# Compute the similarity between the titles\n",
    "cross['sim'] = cross.apply(lambda x: js(x['Title'], x['cit_name']), axis=1)\n",
    "\n",
    "# Mark as matches those with a similarity greater than 0.7\n",
    "found = cross[cross['sim']>0.7]\n",
    "matched_scholar = set(found['scholar_id'].values)\n",
    "\n",
    "# Keep only those were not found\n",
    "miss = cit_scholar[~cit_scholar['scholar_id'].isin(matched_scholar)]\\\n",
    "       .dropna(subset=[\"cit_name\"])[['pub_id', 'cit_name', 'cit_authors', 'cit_venue']]\n",
    "\n",
    "#Joins the citations with the articles to find which are citing\n",
    "missing_citations = miss.merge(pub_scholar, how='inner')[['pub_id', 'cit_name', 'cit_authors', 'cit_venue', 'title']]\n",
    "\n",
    "# Store the missing citations in a CSV file\n",
    "missing_citations.to_csv('missing_simple.csv', sep=\";\")\n",
    "\n",
    "missing_citations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de0bb02",
   "metadata": {},
   "source": [
    "## More accurate approach\n",
    "Matching the title of the citation and the title of the cited article.\n",
    "\n",
    "It is a similarity join on two conditions: JS (cited article title) > 0.5 AND JS (citation title)  0.7\n",
    "\n",
    "\n",
    "Every row of the output dataframe contains:\n",
    "* **cit_name**: title of the missing citation;\n",
    "* **cit_authors**: authors of the missing citation;\n",
    "* **cit_venue**: venue of the missing citation;\n",
    "* **title**: title of the article on Scopus on which the citation is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7123e996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert here your surname\n",
    "author_surname = 'Gagliardelli'\n",
    "\n",
    "# Scopus citations\n",
    "df = pd.read_csv('scopus.csv', sep=\",\")\n",
    "\n",
    "# Takes the list of the references (this field contains all the bibliography entries separated by a semicolon)\n",
    "df['References'] = df['References'].str.split(';')\n",
    "\n",
    "# Create a row for every citation\n",
    "df1 = df.explode('References')\n",
    "\n",
    "# Keep only those in which appear the specified surname\n",
    "df2 = df1[df1['References'].str.contains(author_surname)]\\\n",
    "      [['Authors', 'Title', 'Year', 'Source title', 'DOI', 'References']]\n",
    "\n",
    "# Add an identifier to each citation\n",
    "df2 = df2.reset_index().rename({'index':'scopus_id'}, axis=1)\n",
    "\n",
    "# Keep only: citation id, title, data of the cited article\n",
    "cit_scopus = df2[['scopus_id', 'Title', 'References']]\\\n",
    "             .rename({'Title':'scopus_cit_title', 'References':'scopus_art_title'}, axis=1)\n",
    "\n",
    "# Scholar citations\n",
    "cit_scholar = pd.read_csv('cit_scholar.csv')\n",
    "\n",
    "# Add a unique identifier\n",
    "cit_scholar = cit_scholar.reset_index().rename({'index':'scholar_id'}, axis=1)\n",
    "cit_scholar2 = cit_scholar.rename({'cit_name': 'scholar_cit_title'}, axis=1)\n",
    "\n",
    "# Scopus publications\n",
    "pub_scholar = pd.read_csv('pub_scholar.csv')\n",
    "\n",
    "# For every publication generates a string similar to those of Scopus, with authors, title and venue\n",
    "pub_scholar['scholar_art_title'] = pub_scholar['authors']+\" \"+pub_scholar['title']+\" \"+pub_scholar['venue']\n",
    "\n",
    "# Add the data of the cited article\n",
    "# As for scopus keeps only: citation id, title, data of the cited article\n",
    "cit_scholar2 = cit_scholar2.merge(pub_scholar, how='inner')[['scholar_id', 'scholar_art_title', 'scholar_cit_title']]\n",
    "\n",
    "# Perform the cartesian product\n",
    "cross = cit_scopus.merge(cit_scholar2, how='cross')\n",
    "\n",
    "# Computes the Jaccard Similarity between the title of the citations and the titles of the cited work\n",
    "cross['sim_art'] = cross.apply(lambda x: js(x['scopus_art_title'], x['scholar_art_title']), axis=1)\n",
    "cross['sim_cit'] = cross.apply(lambda x: js(x['scopus_cit_title'], x['scholar_cit_title']), axis=1)\n",
    "\n",
    "# Keep as matches the citations in which the titles of the articles have a js > 0.5 and\n",
    "# the titles of the cited articles have a JS > 0.7\n",
    "found = cross[(cross['sim_art']>0.5) & (cross['sim_cit']>0.7)]\n",
    "\n",
    "# Extract the ids of solved citations\n",
    "matched_scholar = set(found['scholar_id'].values)\n",
    "\n",
    "# Removes solved citations (i.e. keep the missing on scopus)\n",
    "miss = cit_scholar[~cit_scholar['scholar_id'].isin(matched_scholar)]\\\n",
    "       .dropna(subset=[\"cit_name\"])[['pub_id', 'cit_name', 'cit_authors', 'cit_venue']]\n",
    "\n",
    "# Add the title of cited article\n",
    "missing_citations = miss.merge(pub_scholar, how='inner')[['pub_id', 'cit_name', 'cit_authors', 'cit_venue', 'title']]\n",
    "\n",
    "# Store the result in a csv file\n",
    "missing_citations.to_csv('missing_accurate.csv', sep=\";\")\n",
    "missing_citations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
